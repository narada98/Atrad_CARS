{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_ranking as tfr\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.load(\"D:/dev work/recommender systems/Atrad_CARS/data/movie_lens/ratings_train\").cache() #data\\ratings_train\n",
    "test_ds = tf.data.Dataset.load(\"D:/dev work/recommender systems/Atrad_CARS/data/movie_lens/ratings_test\").cache()\n",
    "\n",
    "ratings = tf.data.Dataset.load(\"D:/dev work/recommender systems/Atrad_CARS/data/movie_lens/ratings_all\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_title': <tf.Tensor: shape=(), dtype=string, numpy=b'Postman, The (1997)'>,\n",
       " 'timestamp': <tf.Tensor: shape=(), dtype=int64, numpy=885409515>,\n",
       " 'user_rating': <tf.Tensor: shape=(), dtype=float32, numpy=4.0>,\n",
       " 'user_id': <tf.Tensor: shape=(), dtype=string, numpy=b'681'>}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = ratings.map(lambda x: x[\"movie_title\"])\n",
    "\n",
    "unique_movie_titles = np.unique(np.concatenate(list(movies.batch(1000))))\n",
    "unique_movie_user_ids = np.unique(np.concatenate(list(ratings.batch(1_000).map(\n",
    "    lambda x: x[\"user_id\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# Split between train and tests sets, as before.\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n",
    "\n",
    "# We sample 50 lists for each user for the training data. For each list we\n",
    "# sample 5 movies from the movies the user rated.\n",
    "train = tfrs.examples.movielens.sample_listwise(\n",
    "    train,\n",
    "    num_list_per_user=50,\n",
    "    num_examples_per_list=5,\n",
    "    seed=42\n",
    ")\n",
    "test = tfrs.examples.movielens.sample_listwise(\n",
    "    test,\n",
    "    num_list_per_user=1,\n",
    "    num_examples_per_list=5,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, loss):\n",
    "    super().__init__()\n",
    "    embedding_dimension = 32\n",
    "\n",
    "    # Compute embeddings for users.\n",
    "    self.user_embeddings = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=unique_user_ids),\n",
    "      tf.keras.layers.Embedding(len(unique_user_ids) + 2, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute embeddings for movies.\n",
    "    self.movie_embeddings = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=unique_movie_titles),\n",
    "      tf.keras.layers.Embedding(len(unique_movie_titles) + 2, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute predictions.\n",
    "    self.score_model = tf.keras.Sequential([\n",
    "      # Learn multiple dense layers.\n",
    "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "      # Make rating predictions in the final layer.\n",
    "      tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    self.task = tfrs.tasks.Ranking(\n",
    "      loss=loss,\n",
    "      metrics=[\n",
    "        tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\"),\n",
    "        tf.keras.metrics.RootMeanSquaredError()\n",
    "      ]\n",
    "    )\n",
    "\n",
    "  def call(self, features):\n",
    "    # We first convert the id features into embeddings.\n",
    "    # User embeddings are a [batch_size, embedding_dim] tensor.\n",
    "    user_embeddings = self.user_embeddings(features[\"user_id\"])\n",
    "\n",
    "    # Movie embeddings are a [batch_size, num_movies_in_list, embedding_dim]\n",
    "    # tensor.\n",
    "    movie_embeddings = self.movie_embeddings(features[\"movie_title\"])\n",
    "\n",
    "    # We want to concatenate user embeddings with movie emebeddings to pass\n",
    "    # them into the ranking model. To do so, we need to reshape the user\n",
    "    # embeddings to match the shape of movie embeddings.\n",
    "    list_length = features[\"movie_title\"].shape[1]\n",
    "    user_embedding_repeated = tf.repeat(\n",
    "        tf.expand_dims(user_embeddings, 1), [list_length], axis=1)\n",
    "\n",
    "    # Once reshaped, we concatenate and pass into the dense layers to generate\n",
    "    # predictions.\n",
    "    concatenated_embeddings = tf.concat(\n",
    "        [user_embedding_repeated, movie_embeddings], 2)\n",
    "\n",
    "    return self.score_model(concatenated_embeddings)\n",
    "\n",
    "  def compute_loss(self, features, training=False):\n",
    "    labels = features.pop(\"user_rating\")\n",
    "\n",
    "    scores = self(features)\n",
    "\n",
    "    return self.task(\n",
    "        labels=labels,\n",
    "        predictions=tf.squeeze(scores, axis=-1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_model = RankingModel(tf.keras.losses.MeanSquaredError())\n",
    "mse_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 58ms/step - ndcg_metric: 0.8537 - root_mean_squared_error: 2.7801 - loss: 8.0447 - regularization_loss: 0.0000e+00 - total_loss: 8.0447\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 0s 54ms/step - ndcg_metric: 0.8586 - root_mean_squared_error: 1.6623 - loss: 2.5052 - regularization_loss: 0.0000e+00 - total_loss: 2.5052\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 0s 55ms/step - ndcg_metric: 0.8655 - root_mean_squared_error: 1.1414 - loss: 1.3054 - regularization_loss: 0.0000e+00 - total_loss: 1.3054\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 0s 61ms/step - ndcg_metric: 0.8711 - root_mean_squared_error: 1.1368 - loss: 1.2929 - regularization_loss: 0.0000e+00 - total_loss: 1.2929\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 0s 54ms/step - ndcg_metric: 0.8759 - root_mean_squared_error: 1.1278 - loss: 1.2722 - regularization_loss: 0.0000e+00 - total_loss: 1.2722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e30bd997f0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_model.fit(cached_train, epochs= 5 , verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atrad Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolios = tf.data.Dataset.load(\"../../data/portfolios_tfds_lists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list_ds = tf.data.Dataset.load(\"D:/dev work/recommender systems/Atrad_CARS/data/train_lists_ds\").cache()\n",
    "test_list_ds = tf.data.Dataset.load(\"D:/dev work/recommender systems/Atrad_CARS/data/test_lists_ds\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CDSACCNO': <tf.Tensor: shape=(), dtype=string, numpy=b'RPS-23479-LI/00'>,\n",
       " 'STOCKCODE': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'CIC', b'AMSL', b'RAL', b'REG', b'AEL'], dtype=object)>,\n",
       " 'UNIX_TS': <tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       " array([1.6475418e+09, 1.6681050e+09, 1.7100954e+09, 1.7030970e+09,\n",
       "        1.6655994e+09], dtype=float32)>,\n",
       " 'GICS': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
       " array([b'Materials', b'Health Care Equipment & Services',\n",
       "        b'Food Beverage & Tobacco', b'Consumer Durables & Apparel',\n",
       "        b'Capital Goods'], dtype=object)>,\n",
       " 'STOCKNAME': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
       " array([b'C I C HOLDINGS PLC', b'ASIRI SURGICAL HOSPITAL PLC',\n",
       "        b'RENUKA AGRI FOODS PLC', b'REGNIS (LANKA) PLC',\n",
       "        b'ACCESS ENGINEERING PLC'], dtype=object)>,\n",
       " 'RATING': <tf.Tensor: shape=(5,), dtype=float32, numpy=array([4., 2., 5., 5., 2.], dtype=float32)>}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_list_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_ids = portfolios.batch(10000).map(lambda x: x[\"STOCKCODE\"])\n",
    "item_names = portfolios.batch(10000).map(lambda x: x[\"STOCKNAME\"])\n",
    "\n",
    "user_ids = portfolios.batch(10000).map(lambda x: x[\"CDSACCNO\"])\n",
    "\n",
    "items_ids = portfolios.batch(10000).map(lambda x: x[\"STOCKCODE\"])\n",
    "item_names = portfolios.batch(10000).map(lambda x: x[\"STOCKNAME\"])\n",
    "item_GICS = portfolios.batch(10000).map(lambda x: x[\"GICS\"])\n",
    "\n",
    "user_ids = portfolios.batch(10000).map(lambda x: x[\"CDSACCNO\"])\n",
    "\n",
    "unique_item_ids = np.unique(np.concatenate(list(items_ids)))\n",
    "unique_item_names = np.unique(np.concatenate(list(item_names)))\n",
    "unique_item_gics = np.unique(np.concatenate(list(item_GICS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from user_embedding import UserModel\n",
    "\n",
    "# class UserModel(tf.keras.Model):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         unique_user_ids, \n",
    "#         ):\n",
    "\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.unique_user_ids = unique_user_ids\n",
    "        \n",
    "#         self.embed_user_id = tf.keras.Sequential([\n",
    "#             tf.keras.layers.StringLookup(\n",
    "#                 vocabulary = self.unique_user_ids,\n",
    "#                 mask_token = None\n",
    "#             ),\n",
    "#             tf.keras.layers.Embedding(\n",
    "#                 input_dim = len(self.unique_user_ids)+1,\n",
    "#                 output_dim = 32\n",
    "#             )\n",
    "#         ])\n",
    "\n",
    "    \n",
    "#     def call(self, inputs):\n",
    "\n",
    "#         (user_id) = inputs  #, timestamp\n",
    "#         return self.embed_user_id(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from user_embedding import UserModel\n",
    "from item_embedding import ItemModel\n",
    "\n",
    "class RankingModel_trad(tfrs.Model):\n",
    "\n",
    "  def __init__(self, portfolios ,loss):\n",
    "    super().__init__()\n",
    "\n",
    "    embedding_dimension = 32,\n",
    "    self.loss = loss\n",
    "    \n",
    "    self.portfolios = portfolios\n",
    "\n",
    "    self.items_ids = self.portfolios.batch(10000).map(lambda x: x[\"STOCKCODE\"])\n",
    "    self.item_GICS = self.portfolios.batch(10000).map(lambda x: x[\"GICS\"])\n",
    "    self.unique_item_ids = np.unique(np.concatenate(list(self.items_ids)))\n",
    "    self.unique_item_gics = np.unique(np.concatenate(list(self.item_GICS)))\n",
    "\n",
    "    self.user_ids = self.portfolios.batch(10000).map(lambda x: x[\"CDSACCNO\"])\n",
    "    self.unique_user_ids = np.unique(np.concatenate(list(self.user_ids)))\n",
    "\n",
    "\n",
    "    self.user_embeddings = UserModel(\n",
    "      unique_user_ids = unique_user_ids\n",
    "    )\n",
    "\n",
    "    ## Compute embeddings for movies.\n",
    "    # self.embed_item_code = tf.keras.Sequential([\n",
    "    #   tf.keras.layers.StringLookup(\n",
    "    #     vocabulary= self.unique_item_ids),\n",
    "    #   tf.keras.layers.Embedding(len(unique_item_ids) + 2, embedding_dimension)\n",
    "    # ])\n",
    "\n",
    "    # self.embed_items_gics = tf.keras.Sequential([\n",
    "    #         tf.keras.layers.StringLookup(\n",
    "    #             vocabulary = self.unique_item_gics,\n",
    "    #             mask_token = None\n",
    "    #         ),\n",
    "    #         tf.keras.layers.Embedding(\n",
    "    #             input_dim = len(self.unique_item_gics)+1,\n",
    "    #             output_dim = 16 #len(unique_item_gics)\n",
    "    #         )\n",
    "    #     ])\n",
    "\n",
    "    self.item_embeddings = ItemModel(\n",
    "      unique_item_ids = self.unique_item_ids,\n",
    "      unique_item_gics = self.unique_item_gics\n",
    "    )\n",
    "\n",
    "    # Compute predictions.\n",
    "    self.score_model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    self.task = tfrs.tasks.Ranking(\n",
    "      loss=loss,\n",
    "      metrics=[\n",
    "        tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\"),\n",
    "        tf.keras.metrics.RootMeanSquaredError()\n",
    "      ]\n",
    "    )\n",
    "\n",
    "  def call(self, features):\n",
    "    user_embeddings = self.user_embeddings(features[\"CDSACCNO\"])\n",
    "    # code_embeddings = self.embed_item_code(features[\"STOCKCODE\"])\n",
    "    # gics_embeddings = self.embed_items_gics(features['GICS'])\n",
    "\n",
    "    item_embeddings = self.item_embeddings((\n",
    "      features['STOCKCODE'],\n",
    "      features['GICS']\n",
    "      ))\n",
    "\n",
    "    print()\n",
    "    list_length = features[\"STOCKCODE\"].shape[1]\n",
    "    user_embedding_repeated = tf.repeat(\n",
    "        tf.expand_dims(user_embeddings, 1), [list_length], axis=1)\n",
    "\n",
    "    concatenated_embeddings = tf.concat(\n",
    "        [user_embedding_repeated, item_embeddings], 2)\n",
    "\n",
    "    return self.score_model(concatenated_embeddings)\n",
    "\n",
    "  def compute_loss(self, features, training=False):\n",
    "    labels = features.pop(\"RATING\")\n",
    "\n",
    "    scores = self(features)\n",
    "\n",
    "    return self.task(\n",
    "        labels=labels,\n",
    "        predictions=tf.squeeze(scores, axis=-1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6950"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "cached_train_trad = train_list_ds.shuffle(100_000).batch(128).cache()\n",
    "cached_test_trad = test_list_ds.batch(128).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(cached_train_trad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "trad_model = RankingModel_trad(\n",
    "    portfolios = portfolios,\n",
    "    loss = tf.keras.losses.MeanSquaredError()\n",
    "    )\n",
    "trad_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\bpadmin\\anaconda3\\envs\\atrad_cars_v2\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\bpadmin\\anaconda3\\envs\\atrad_cars_v2\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\bpadmin\\anaconda3\\envs\\atrad_cars_v2\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\bpadmin\\anaconda3\\envs\\atrad_cars_v2\\lib\\site-packages\\tensorflow_recommenders\\models\\base.py\", line 68, in train_step\n        loss = self.compute_loss(inputs, training=True)\n    File \"C:\\Users\\naradaw\\AppData\\Local\\Temp\\ipykernel_23188\\1574606882.py\", line 88, in compute_loss\n        scores = self(features)\n    File \"c:\\Users\\bpadmin\\anaconda3\\envs\\atrad_cars_v2\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\naradaw\\AppData\\Local\\Temp\\__autograph_generated_filekbjyz1n_.py\", line 11, in tf__call\n        item_embeddings = ag__.converted_call(ag__.ld(item_embeddings), ((ag__.ld(features)['STOCKCODE'], ag__.ld(features)['GICS']),), None, fscope)\n\n    UnboundLocalError: Exception encountered when calling layer 'ranking_model_trad_5' (type RankingModel_trad).\n    \n    in user code:\n    \n        File \"C:\\Users\\naradaw\\AppData\\Local\\Temp\\ipykernel_23188\\1574606882.py\", line 70, in call  *\n            item_embeddings = item_embeddings((\n    \n        UnboundLocalError: local variable 'item_embeddings' referenced before assignment\n    \n    \n    Call arguments received by layer 'ranking_model_trad_5' (type RankingModel_trad):\n      • features={'CDSACCNO': 'tf.Tensor(shape=(None,), dtype=string)', 'STOCKCODE': 'tf.Tensor(shape=(None, 5), dtype=string)', 'UNIX_TS': 'tf.Tensor(shape=(None, 5), dtype=float32)', 'GICS': 'tf.Tensor(shape=(None, 5), dtype=string)', 'STOCKNAME': 'tf.Tensor(shape=(None, 5), dtype=string)'}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[166], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrad_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_train_trad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bpadmin\\anaconda3\\envs\\atrad_cars_v2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filefvzal0mz.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bpadmin\\anaconda3\\envs\\atrad_cars_v2\\lib\\site-packages\\tensorflow_recommenders\\models\\base.py:68\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom train step using the `compute_loss` method.\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 68\u001b[0m   loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     70\u001b[0m   \u001b[38;5;66;03m# Handle regularization losses as well.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m   regularization_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses)\n",
      "Cell \u001b[1;32mIn[159], line 88\u001b[0m, in \u001b[0;36mRankingModel_trad.compute_loss\u001b[1;34m(self, features, training)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, features, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     86\u001b[0m   labels \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRATING\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 88\u001b[0m   scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask(\n\u001b[0;32m     91\u001b[0m       labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m     92\u001b[0m       predictions\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39msqueeze(scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     93\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filekbjyz1n_.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m     10\u001b[0m user_embeddings \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39muser_embeddings, (ag__\u001b[38;5;241m.\u001b[39mld(features)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCDSACCNO\u001b[39m\u001b[38;5;124m'\u001b[39m],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 11\u001b[0m item_embeddings \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(item_embeddings), ((ag__\u001b[38;5;241m.\u001b[39mld(features)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTOCKCODE\u001b[39m\u001b[38;5;124m'\u001b[39m], ag__\u001b[38;5;241m.\u001b[39mld(features)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGICS\u001b[39m\u001b[38;5;124m'\u001b[39m]),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)()\n\u001b[0;32m     13\u001b[0m list_length \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(features)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTOCKCODE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: in user code:\n\n    File \"c:\\Users\\bpadmin\\anaconda3\\envs\\atrad_cars_v2\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\bpadmin\\anaconda3\\envs\\atrad_cars_v2\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\bpadmin\\anaconda3\\envs\\atrad_cars_v2\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\bpadmin\\anaconda3\\envs\\atrad_cars_v2\\lib\\site-packages\\tensorflow_recommenders\\models\\base.py\", line 68, in train_step\n        loss = self.compute_loss(inputs, training=True)\n    File \"C:\\Users\\naradaw\\AppData\\Local\\Temp\\ipykernel_23188\\1574606882.py\", line 88, in compute_loss\n        scores = self(features)\n    File \"c:\\Users\\bpadmin\\anaconda3\\envs\\atrad_cars_v2\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\naradaw\\AppData\\Local\\Temp\\__autograph_generated_filekbjyz1n_.py\", line 11, in tf__call\n        item_embeddings = ag__.converted_call(ag__.ld(item_embeddings), ((ag__.ld(features)['STOCKCODE'], ag__.ld(features)['GICS']),), None, fscope)\n\n    UnboundLocalError: Exception encountered when calling layer 'ranking_model_trad_5' (type RankingModel_trad).\n    \n    in user code:\n    \n        File \"C:\\Users\\naradaw\\AppData\\Local\\Temp\\ipykernel_23188\\1574606882.py\", line 70, in call  *\n            item_embeddings = item_embeddings((\n    \n        UnboundLocalError: local variable 'item_embeddings' referenced before assignment\n    \n    \n    Call arguments received by layer 'ranking_model_trad_5' (type RankingModel_trad):\n      • features={'CDSACCNO': 'tf.Tensor(shape=(None,), dtype=string)', 'STOCKCODE': 'tf.Tensor(shape=(None, 5), dtype=string)', 'UNIX_TS': 'tf.Tensor(shape=(None, 5), dtype=float32)', 'GICS': 'tf.Tensor(shape=(None, 5), dtype=string)', 'STOCKNAME': 'tf.Tensor(shape=(None, 5), dtype=string)'}\n"
     ]
    }
   ],
   "source": [
    "trad_model.fit(cached_train_trad, epochs= 5 , verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(cached_train_trad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atrad_cars_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
